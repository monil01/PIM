Several recent studies explore the architectural scope and application domains of processing-in-memory.

\subsection{Architectural Scope of PIM}

Processing in the memory array is addressed in several studies~\cite{03,06,13,15,20,29}. Leitersdorf et al. \cite{13} propose to improve in-memory multiplication with partition-based computation techniques for broadcasting and moving data within partitions. The authors exploit ReRAM's voltage controlled variable resistance to support logic gates. Long et al. \cite{20} proposes another ReRAM based design particularly for Recurrent Neural Network (RNN) acceleration. The study proposes crossbar sub-arrays for vmatrix vector multiplication, multiplier sub-arrays for element-wise operations and special function units for nonlinear functions. Peng et al. \cite{15} present a new mapping method and data flow to maximize reuse of weight and input data on a 8-bit ReRAM based PIM architecture. 
Lin et al. \cite{31} present PIM design proposing optimization at the row buffer level. The authors introduce a \textit{Population Count Engine} which works on the data inside the sense amplifier and store the results back to the sense amplifier. Majority of the studies adopt near memory compute unit equipped with simple core(s) \cite{01,02,05,11,12,17,30,32,33,34,35}. Lee et al. \cite{12} develops an industrial prototype of a PIM design that can seamlessly work with a variety of commercial processors without requiring any changes on the applications. The proposed PIM execution unit has a five-stage pipeline, 16-wide SIMD FPU, registers and controller. This design is based on a HBM stack which is fabricated with a 20nm technology process. PimCaffe \cite{16} develops a PIM near memory design based on multiple FPGAs, implementing SIMD and systolic array compute engines for vector and matrix multiplications.       

\subsection{Application Domain}

The vast majority of the studies attempts to accelerate application segments from Machine Learning, AI and Neural Network domain \cite{03,04,08,09,10,11,14,15,16,18,19,20,22,29,31}. Since applications from these domains frequently use matrix vector multiplication operations, where the coefficients can be naturally mapped to the word-lines (WL) and bit-lines (BL) and use the data array to compute the dot product in the each cell then eventually accumulating them along the source-lines (SL) \cite{15,20}. In another approach few studies use the logic die of 3D stacked memories to place Processing Elements (PE) directly beneath the DRAM dies with shorter distance to the coefficient data for faster computation in each layer [09,22]. Imani et al. \cite{6} report to have achieved graph processing and query processing capabilities along with machine learning acceleration with processing-in-memory. Huang et al. \cite{33} propose a heterogeneous PIM design incorporating memristors and CMOS based technologies, in order to accommodate the heterogeneity requirements of graph applications. DAMOV~\cite{71} is a PIM simulation infrastructure based ZSim[] and Ramulator[] with offloading support. The authors develops a rigorous workload characterization methodology and quantify data movement bottleneck on an extensive set of applications and functions.     

